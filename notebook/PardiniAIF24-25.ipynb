{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e26e7d8",
   "metadata": {},
   "source": [
    "# Project AIF 24-25\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57e978",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "This notebook report is part of the AIF 24-25 course of the Master's Degree in Computer Science (Artificial Intelligence curriculum) at the University of Pisa.  \n",
    "It is built upon code from my thesis work, \"The Hyperview Challenge: proposal of a Machine Learning model to improve the state of the art in estimating soil parameters from hyperspectral images\" based on the [Hyperview Challenge](#https://platform.ai4eo.eu/seeing-beyond-the-visible-permanent). \n",
    "\n",
    "In this project, we focus on comparing different Optuna samplers—particularly Optuna's NSGAIISampler (genetic-based), but also TPE, Random, to assess their performance in tuning a Random Forest regressor for predicting soil parameters. By systematically analyzing both **efficiency** (CPU time, memory usage) and **effectiveness** (MSE improvements).\n",
    "\n",
    "Below, we present a compact report including **Related Works**, **Methodologies**, **Assessment**, **Conclusions**, and an **Appendix**.  \n",
    "We also provide the full code and additional documentation in the [GitHub repository link](https://github.com/alessioPardiniJob/AIF24-25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478dc0cb",
   "metadata": {},
   "source": [
    "### 2. Related Work\n",
    "\n",
    "Hyperparameter optimization plays a crucial role in developing efficient machine learning models. Akiba et al. (2019) presented Optuna, a hyperparameter optimization framework, demonstrating that it outperforms traditional frameworks such as Hyperopt and SMAC on challenging problems such as Combined Algorithm Selection and Hyperparameter Optimization (CASH) with complex search spaces [link to the [paper](#https://arxiv.org/abs/1907.10902)].\n",
    "\n",
    "\n",
    "Chiba (2024) evaluated open-source optimization algorithms, including TPE, CMA-ES, and NSGA-II, for optimizing the composition of functionally graded materials (FGMs) under thermal variations. Using the Optuna framework, the study found CMA-ES to be most effective in reducing residual thermal stress, while TPE showed faster convergence but was less effective than evolutionary algorithms [link to the [paper](#https://doi.org/10.1504/IJCAET.2024.10063825)].\n",
    "\n",
    "Most existing literature has focused on the NSGA-II sampler for multi-objective optimization, as detailed by Haiping et al. (2023) [link to the [paper](#https://doi.org/10.1007/s10462-023-10526-z)], leaving a gap in systematic evaluations for single-objective optimization.\n",
    "\n",
    "Our study aims to address this gap by proposing an evaluation of different configurations of the NSGA-II sampler within single-objective optimization (focused on rMSE reduction). This includes an analysis of its computational performance (CPU time and memory usage) and effectiveness (measured by Mean Squared Error reduction). This study broadens the existing landscape of practical applications of samplers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ddca2",
   "metadata": {},
   "source": [
    "### 3. Methodologies\n",
    "\n",
    "#### 3.1 Setup and Constants\n",
    "Notebook initialization with standard imports, third-party libraries, and essential constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444c5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Ottiene il percorso della directory del notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Aggiunge il percorso relativo della directory 'utils' al sys.path\n",
    "utils_path = os.path.abspath(os.path.join(notebook_dir, '../utils'))\n",
    "sys.path.append(utils_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d458f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used in the notebook\n",
    "DEBUG = True\n",
    "AUGMENT_CONSTANT_RF = 1\n",
    "LABEL_NAMES = [\"P2O5\", \"K\", \"Mg\", \"pH\"]\n",
    "LABEL_MAXS = np.array([325.0, 625.0, 400.0, 7.8])\n",
    "COL_IX = [0, 1, 2, 3]\n",
    "\n",
    "# Set number of trials based on debug mode\n",
    "n_trials = 3 if DEBUG else 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fd0cc",
   "metadata": {},
   "source": [
    "#### 3.2 Data Loading, Preprocessing, and Cross-Validation Pipeline for Training and Test Sets\n",
    "\n",
    "(For details on data loading and preprocessing, refer to the [GitHub repository](#https://github.com/your-repo-link)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17ec25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training data ..: 100%|██████████| 10/10 [00:00<00:00, 1041.93it/s]\n",
      "Loading augmentation 1 ..: 100%|██████████| 10/10 [00:00<00:00, 671.98it/s]\n",
      "Loading test data ..: 100%|██████████| 10/10 [00:00<00:00, 1049.02it/s]\n",
      "INFO: Preprocessing data ...: 100%|██████████| 10/10 [00:00<00:00, 420.98it/s]\n",
      "INFO: Preprocessing data ...: 100%|██████████| 10/10 [00:00<00:00, 419.48it/s]\n",
      "INFO: Preprocessing data ...: 100%|██████████| 10/10 [00:00<00:00, 435.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the directory and file paths are correctly set for your system\n",
    "train_data_dir = os.path.abspath(os.path.join(notebook_dir, '../data/train_data/train_data'))\n",
    "test_data_dir = os.path.abspath(os.path.join(notebook_dir, '../data/test_data'))\n",
    "gt_data_path = os.path.abspath(os.path.join(notebook_dir, '../data/train_data/train_gt.csv'))\n",
    "\n",
    "\n",
    "# Load training raw data\n",
    "X_train, M_train, y_train, X_aug_train, M_aug_train, y_aug_train = load_data(\n",
    "    train_data_dir, gt_data_path, is_train=True, augment_constant=AUGMENT_CONSTANT_RF, DEBUG=DEBUG, LABEL_MAXS=LABEL_MAXS\n",
    ")\n",
    "\n",
    "# Load test raw data\n",
    "X_test, M_test = load_data(\n",
    "    test_data_dir, gt_file_path=None, is_train=False, DEBUG=DEBUG, LABEL_MAXS=LABEL_MAXS\n",
    ")\n",
    "\n",
    "# Preprocessing the loaded data\n",
    "X_tr_processed_RF, avg_edge_train = preprocess(X_train, M_train)\n",
    "X_aug_processed_RF, avg_edge_train_aug_RF = preprocess(X_aug_train, M_aug_train)\n",
    "X_te_processed_RF, avg_edge_test = preprocess(X_test, M_test)\n",
    "\n",
    "# Select set of labels \n",
    "y_train_col = y_train[:, COL_IX]  \n",
    "y_aug_train_col = y_aug_train[:len(y_train_col)*AUGMENT_CONSTANT_RF, COL_IX]\n",
    "\n",
    "# 5-fold cross validation for training.\n",
    "kfold = KFold(shuffle=True, random_state=2022)\n",
    "kfold.get_n_splits(X_aug_train, y_aug_train_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567a57d",
   "metadata": {},
   "source": [
    "#### 3.3 Objective Function Definition\n",
    "\n",
    "The objective function optimizes the hyperparameters of the [Random Forest](https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    " model using Optuna:\n",
    "- **n_estimators**: Number of trees, suggested in the range [50, 300] with steps of 25.\n",
    "- **max_depth**: Maximum tree depth, suggested in the range [3, 15].\n",
    "- **min_samples_split**: Minimum samples required to split a node, suggested in the range [2, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6baeafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna:\n",
    "    Performs cross-validation with RandomForest and returns the relative MSE (mse_rf / mse_bl) to minimize.\n",
    "    \"\"\"\n",
    "    # Suggested parameters\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300, step=25)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "\n",
    "    # Cross-validation\n",
    "    y_hat_rf_cv, y_hat_bl_cv, y_v_list_cv = [], [], []\n",
    "\n",
    "    for idx, (ix_train, ix_valid) in enumerate(kfold.split(np.arange(len(y_train)), avg_edge_train.astype(int))):\n",
    "        # Merge training data\n",
    "        X_t = np.concatenate((X_tr_processed_RF[ix_train], X_aug_processed_RF[ix_train]), axis=0)\n",
    "        y_t = np.concatenate((y_train_col[ix_train], y_aug_train_col[ix_train]), axis=0)\n",
    "\n",
    "        # Validation data\n",
    "        X_v, y_v = X_tr_processed_RF[ix_valid], y_train_col[ix_valid]\n",
    "        y_v_list_cv.append(y_v)\n",
    "\n",
    "        # Baseline model\n",
    "        baseline = BaselineRegressor()\n",
    "        baseline.fit(X_t, y_t)\n",
    "        y_b = baseline.predict(X_v)\n",
    "        y_hat_bl_cv.append(y_b)\n",
    "\n",
    "        # RandomForest model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_t, y_t)\n",
    "        y_hat_rf_cv.append(model.predict(X_v))\n",
    "\n",
    "    # Compute relative MSE for each fold\n",
    "    total_score = 0.0\n",
    "\n",
    "    for y_hat, y_b, y_v in zip(y_hat_rf_cv, y_hat_bl_cv, y_v_list_cv):\n",
    "        fold_score = sum(\n",
    "            mean_squared_error(y_v[:, i] * LABEL_MAXS[i], y_hat[:, i] * LABEL_MAXS[i]) /\n",
    "            mean_squared_error(y_v[:, i] * LABEL_MAXS[i], y_b[:, i] * LABEL_MAXS[i])\n",
    "            for i in COL_IX\n",
    "        ) / len(COL_IX)\n",
    "        total_score += fold_score\n",
    "\n",
    "    return total_score / len(y_hat_rf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b7ab7",
   "metadata": {},
   "source": [
    "#### 3.4 Execution of the Study in a Subprocess\n",
    "\n",
    "To ensure isolation and accurate measurement of resource utilization, each Optuna study is executed in a separate child process. The `run_study_in_subprocess` function handles the study's execution, collects metrics related to execution time, CPU usage, and memory consumption, and returns the primary results, including the optimal parameters identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cae21258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_study_in_subprocess(sampler, sampler_name, n_trials):\n",
    "    \"\"\"\n",
    "    Function that:\n",
    "      - Runs in a child process.\n",
    "      - Measures CPU, memory, and time usage of the child process only.\n",
    "      - Executes the Optuna study with the specified sampler.\n",
    "      - Returns a dictionary with results (best trial, timings, etc.).\n",
    "    \"\"\"\n",
    "    gc.collect()  # Force garbage collection for safety\n",
    "\n",
    "    proc = psutil.Process()  # Create a \"Process\" instance for the current child process\n",
    "\n",
    "    # Initial measurements\n",
    "    start_cpu, start_mem, start_time = proc.cpu_times(), proc.memory_info().rss, time.time()\n",
    "\n",
    "    # Create and run the study\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "\n",
    "    # Final measurements\n",
    "    end_time, end_cpu, end_mem = time.time(), proc.cpu_times(), proc.memory_info().rss\n",
    "\n",
    "    return {\n",
    "        \"sampler_name\": sampler_name,\n",
    "        \"best_value\": study.best_trial.value,\n",
    "        \"best_params\": study.best_trial.params,\n",
    "        \"elapsed_time_sec\": end_time - start_time,\n",
    "        \"cpu_time_used_sec\": (end_cpu.user - start_cpu.user) + (end_cpu.system - start_cpu.system),\n",
    "        \"memory_used_bytes\": end_mem - start_mem\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57d8f6",
   "metadata": {},
   "source": [
    "#### 3.5 Sampler Configuration\n",
    "\n",
    "We define configurations for **samplers** in Optuna, including variants of `NSGAIISampler` and standard samplers like `TPESampler` and `RandomSampler`.\n",
    "\n",
    "##### NSGAIISampler Configurations\n",
    "\n",
    "- **Pop5_Cr90_Mut10**:\n",
    "  - `population_size=5`, `crossover_prob=0.9`, `mutation_prob=0.1`\n",
    "  - **Expectations**: Lower resource consumption, accepting a trade-off in quality.\n",
    "\n",
    "- **Pop10_Cr85_Mut15**:\n",
    "  - `population_size=10`, `crossover_prob=0.85`, `mutation_prob=0.15`\n",
    "  - **Expectations**: Higher resource consumption, better accuracy compared to Pop5.\n",
    "\n",
    "- **Pop10_SPXCrossover_Cr80_Mut20**:\n",
    "  - `population_size=10`, `crossover=SPXCrossover()`, `crossover_prob=0.8`, `mutation_prob=0.2`\n",
    "  - **Expectations**: Increased diversity with quality solutions, slightly higher computational costs.\n",
    "\n",
    "---\n",
    "\n",
    "##### Other Samplers\n",
    "\n",
    "- **Configurations**: `TPESampler()`, `RandomSampler()`\n",
    "  - **Expectations**:\n",
    "    - **TPESampler**: Good balance between effectiveness and resource usage.\n",
    "    - **RandomSampler**: Maximum efficiency but lower effectiveness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74a5f994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2720656/2857723655.py:8: ExperimentalWarning: SPXCrossover is experimental (supported from v3.0.0). The interface can change in the future.\n",
      "  (NSGAIISampler(population_size=10, crossover_prob=0.8, mutation_prob=0.2, crossover=SPXCrossover()), \"NSGAIISampler_customPop10_SPXCrossover_Cr80_Mut20\"),\n",
      "[I 2025-01-23 15:44:50,161] A new study created in memory with name: no-name-6ea4b717-aace-4abc-bb6e-8c95e33f5bf2\n",
      "[I 2025-01-23 15:44:56,239] Trial 1 finished with value: 1.1589597679263397 and parameters: {'n_estimators': 75, 'max_depth': 12, 'min_samples_split': 6}. Best is trial 1 with value: 1.1589597679263397.\n",
      "[I 2025-01-23 15:44:57,248] Trial 2 finished with value: 1.1596876563711382 and parameters: {'n_estimators': 225, 'max_depth': 14, 'min_samples_split': 7}. Best is trial 1 with value: 1.1589597679263397.\n",
      "[I 2025-01-23 15:44:57,250] Trial 0 finished with value: 1.15633747315337 and parameters: {'n_estimators': 125, 'max_depth': 13, 'min_samples_split': 7}. Best is trial 0 with value: 1.15633747315337.\n",
      "[I 2025-01-23 15:44:57,371] A new study created in memory with name: no-name-3b5011b8-0a6d-4172-8732-0f84dd08775b\n",
      "[I 2025-01-23 15:45:02,473] Trial 0 finished with value: 1.2660277957442119 and parameters: {'n_estimators': 75, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 0 with value: 1.2660277957442119.\n",
      "[I 2025-01-23 15:45:03,151] Trial 2 finished with value: 1.4968287348065914 and parameters: {'n_estimators': 100, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 0 with value: 1.2660277957442119.\n",
      "[I 2025-01-23 15:45:03,222] Trial 1 finished with value: 1.1785608623054087 and parameters: {'n_estimators': 175, 'max_depth': 7, 'min_samples_split': 8}. Best is trial 1 with value: 1.1785608623054087.\n",
      "[I 2025-01-23 15:45:03,329] A new study created in memory with name: no-name-f1dd3a67-6961-4143-9f8c-c7829292a255\n",
      "[I 2025-01-23 15:45:08,988] Trial 1 finished with value: 1.4230434675579746 and parameters: {'n_estimators': 75, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 1 with value: 1.4230434675579746.\n",
      "[I 2025-01-23 15:45:09,460] Trial 2 finished with value: 1.1036873626205075 and parameters: {'n_estimators': 125, 'max_depth': 5, 'min_samples_split': 8}. Best is trial 2 with value: 1.1036873626205075.\n",
      "[I 2025-01-23 15:45:09,553] Trial 0 finished with value: 1.1769993900021443 and parameters: {'n_estimators': 175, 'max_depth': 9, 'min_samples_split': 7}. Best is trial 2 with value: 1.1036873626205075.\n",
      "[I 2025-01-23 15:45:09,668] A new study created in memory with name: no-name-1836a0c6-fef1-4176-99f2-c93e74de9d06\n",
      "[I 2025-01-23 15:45:16,084] Trial 1 finished with value: 1.4694977636554696 and parameters: {'n_estimators': 75, 'max_depth': 13, 'min_samples_split': 3}. Best is trial 1 with value: 1.4694977636554696.\n",
      "[I 2025-01-23 15:45:16,895] Trial 2 finished with value: 1.1320608726232184 and parameters: {'n_estimators': 200, 'max_depth': 3, 'min_samples_split': 9}. Best is trial 2 with value: 1.1320608726232184.\n",
      "[I 2025-01-23 15:45:16,994] Trial 0 finished with value: 1.5908285521664396 and parameters: {'n_estimators': 200, 'max_depth': 9, 'min_samples_split': 2}. Best is trial 2 with value: 1.1320608726232184.\n",
      "[I 2025-01-23 15:45:17,104] A new study created in memory with name: no-name-a0243cdf-a0a9-47c9-97e1-03d8fa5a4f6d\n",
      "[I 2025-01-23 15:45:23,090] Trial 1 finished with value: 1.3210134105638365 and parameters: {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 5}. Best is trial 1 with value: 1.3210134105638365.\n",
      "[I 2025-01-23 15:45:25,113] Trial 2 finished with value: 1.1421733114023407 and parameters: {'n_estimators': 200, 'max_depth': 9, 'min_samples_split': 8}. Best is trial 2 with value: 1.1421733114023407.\n",
      "[I 2025-01-23 15:45:25,246] Trial 0 finished with value: 1.1339099713508638 and parameters: {'n_estimators': 250, 'max_depth': 13, 'min_samples_split': 6}. Best is trial 0 with value: 1.1339099713508638.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Sampler configurations\n",
    "    sampler_configurations = [\n",
    "        (NSGAIISampler(population_size=5, crossover_prob=0.9, mutation_prob=0.1), \"NSGAIISampler_customPop5_Cr90_Mut10\"),\n",
    "        (NSGAIISampler(population_size=10, crossover_prob=0.85, mutation_prob=0.15), \"NSGAIISampler_customPop10_Cr85_Mut15\"),\n",
    "        (NSGAIISampler(population_size=10, crossover_prob=0.8, mutation_prob=0.2, crossover=SPXCrossover()), \"NSGAIISampler_customPop10_SPXCrossover_Cr80_Mut20\"),\n",
    "        (TPESampler(), \"TPESampler\"),\n",
    "        (RandomSampler(), \"RandomSampler\")\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Running studies for each sampler configuration\n",
    "    for sampler, sampler_name in sampler_configurations:\n",
    "        parent_conn, child_conn = mp.Pipe()\n",
    "        p = mp.Process(\n",
    "            target=lambda conn, s, name, trials: conn.send(run_study_in_subprocess(s, name, trials)),\n",
    "            args=(child_conn, sampler, sampler_name, n_trials)\n",
    "        )\n",
    "        p.start()\n",
    "        p.join()\n",
    "\n",
    "        results.append(parent_conn.recv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83351e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        sampler_name  best_value  \\\n",
      "0                NSGAIISampler_customPop5_Cr90_Mut10    1.156337   \n",
      "1               NSGAIISampler_customPop10_Cr85_Mut15    1.178561   \n",
      "2  NSGAIISampler_customPop10_SPXCrossover_Cr80_Mut20    1.103687   \n",
      "3                                         TPESampler    1.132061   \n",
      "4                                      RandomSampler    1.133910   \n",
      "\n",
      "                                         best_params  elapsed_time_sec  \\\n",
      "0  {'n_estimators': 125, 'max_depth': 13, 'min_sa...          7.092334   \n",
      "1  {'n_estimators': 175, 'max_depth': 7, 'min_sam...          5.853074   \n",
      "2  {'n_estimators': 125, 'max_depth': 5, 'min_sam...          6.225976   \n",
      "3  {'n_estimators': 200, 'max_depth': 3, 'min_sam...          7.327610   \n",
      "4  {'n_estimators': 250, 'max_depth': 13, 'min_sa...          8.143943   \n",
      "\n",
      "   cpu_time_used_sec  memory_used_bytes  \n",
      "0              28.11           33148928  \n",
      "1              28.58           34082816  \n",
      "2              26.13           33701888  \n",
      "3              42.37           37109760  \n",
      "4              33.66           33267712  \n"
     ]
    }
   ],
   "source": [
    "# Save results to a DataFrame and a text file\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "output_file = \"final_results.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"================= FINAL RESULTS =================\\n\" + df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14122b27",
   "metadata": {},
   "source": [
    "### 4. Assessment\n",
    "\n",
    "#### 4.1 Performance Metrics:\n",
    "\n",
    "- **Relative MSE** \\(**MSE_RF** / **MSE_baseline**\\):  \n",
    "  This metric represents the ratio of the Random Forest’s mean squared error to that of the baseline regressor. A value below 1 indicates that the Random Forest outperforms the baseline model.\n",
    "\n",
    "- **CPU Time**:  \n",
    "  The sum of user and system CPU time (in seconds) is recorded using `psutil` in the dedicated child process.\n",
    "\n",
    "- **Memory Usage**:  \n",
    "  The difference in the Resident Set Size (RSS) before and after execution is measured via `psutil`.\n",
    "\n",
    "#### 4.2 Results Summary:\n",
    "\n",
    "| **Sampler Name**                                | **Best Value** | **Best Parameters**                                                                 | **Elapsed Time (s)** | **CPU Time Used (s)** | **Memory Used (bytes)** |\n",
    "|-------------------------------------------------|----------------|------------------------------------------------------------------------------------|-----------------------|-----------------------|--------------------------|\n",
    "| NSGAIISampler_customPop5_Cr90_Mut10             | 0.858477       | {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 3}                     | 2016.477158           | 171903.91            | 76177408                |\n",
    "| NSGAIISampler_customPop10_Cr85_Mut15            | 0.859063       | {'n_estimators': 125, 'max_depth': 14, 'min_samples_split': 6}                     | 1580.563911           | 134562.86            | 72318976                |\n",
    "| NSGAIISampler_customPop10_SPXCrossover_Cr80_Mut20 | 0.853163       | {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 2}                     | 2048.171755           | 167423.77            | 100864000               |\n",
    "| TPESampler                                      | 0.854925       | {'n_estimators': 300, 'max_depth': 15, 'min_samples_split': 2}                     | 2292.659146           | 196769.72            | 102473728               |\n",
    "| RandomSampler                                   | 0.859254       | {'n_estimators': 150, 'max_depth': 15, 'min_samples_split': 4}                     | 1705.460728           | 144133.91            | 58699776                |\n",
    "\n",
    "The results indicate a consistent performance in terms of effectiveness (RMSE) across all methods, with minimal variations likely due to the limited number of trials constrained by computational resources.  \n",
    "\n",
    "In terms of efficiency, the **NSGA-II** method stands out for its relatively high resource consumption. Specifically, the **NSGAIISampler_customPop10_SPXCrossover_Cr80_Mut20** recorded 167,423 seconds of CPU time and 100 MB of memory usage, making it one of the most resource-intensive configurations. However, the **NSGAIISampler_customPop10_Cr85_Mut15** achieves a better trade-off between computational cost and RMSE performance, requiring less CPU time and memory compared to the SPX crossover variant.  \n",
    "\n",
    "The **TPE Sampler**, while delivering competitive RMSE results, exhibited the highest computational cost, with 196,769 seconds of CPU time and 102 MB of memory usage, rendering it less efficient than both NSGA-II and RandomSampler.\n",
    "\n",
    "### 5. Conclusions\n",
    "\n",
    "This study compared multiple Optuna samplers, focusing on **NSGA-II**, for optimizing a Random Forest model predicting soil parameters from hyperspectral data. While RMSE performance was comparable across samplers, **NSGA-II** balanced performance and resource usage effectively. Simpler alternatives like **TPE** and **RandomSampler** showed different computational trade-offs.  \n",
    "\n",
    "Future work could explore larger trial budgets, alternative machine learning models, or hybrid sampling strategies to further enhance optimization efficiency and accuracy.\n",
    "\n",
    "### 6. Appendix\n",
    "\n",
    "### 6.1 Individual Contributions\n",
    "\n",
    "- **Alessio Pardini**\n",
    "\n",
    "#### 6.2 Relationship with AIF 24-25\n",
    "\n",
    "In this project, we analyzed the performance of Optuna's genetic sampler, NSGAIISampler, connecting it to the topic of genetic algorithms covered in the AIF 24-25 course, specifically applied here to the context of model selection.\n",
    "\n",
    "Theoretically, a genetic algorithm can be viewed as a variant of stochastic beam search inspired by the metaphor of natural selection. It operates on a population of individuals (in this case, potential hyperparameter configurations), evaluates their fitness (here, the rMSE of a Random Forest model), and selects the most promising individuals based on a fitness function. NSGA-II performs selection using non-dominated sorting to group the population into Pareto fronts, prioritizing individuals from the superior front. Within the same front, the crowding distance is used to maintain diversity and avoid population collapse in a specific search space region.\n",
    "\n",
    "Crossover operators are then applied to combine configurations and generate new ones. In our test, we used the default operator and also evaluated SPXCrossover. Finally, mutation operators introduce random changes to hyperparameters. NSGA-II employs polynomial mutation, which introduces controlled random variations while ensuring generated values remain within valid bounds. The mutation probability is adjusted through a specific parameter to regulate its effect.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41de22",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
